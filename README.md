# Pipeline de Enriquecimento de Dados com LLM (GPT-4o-mini)

Este projeto implementa um pipeline de ETL (Extract, Transform, Load) inteligente e escal√°vel, focado na normaliza√ß√£o e enriquecimento sem√¢ntico de uma base de dados de produtos (1M+ registros). A solu√ß√£o utiliza LLMs para descoberta din√¢mica de esquemas e processamento em lote para garantir resili√™ncia.

## üèóÔ∏è Arquitetura do Sistema

A solu√ß√£o foi desenhada para operar de forma modular, utilizando o **AWS S3** como camada de persist√™ncia (Data Lake) e o modelo **GPT-4o-mini** da OpenAI como motor de intelig√™ncia.

### Fluxo de Dados (Pipeline)

O processo est√° dividido em quatro etapas principais, conforme ilustrado no diagrama `Dadosfera ETL LLM.png`:

1.  **Limpeza e Normaliza√ß√£o (`ETL 1`):**
    * **Entrada:** `products_raw.jsonl` (Dados brutos).
    * **Processamento:** Limpeza de strings, tratamento de valores nulos e filtragem inicial.
    * **Sa√≠da:** `products_clean.json` (Armazenado no bucket S3 `dadosfera-datalake`).

2.  **Descoberta de Atributos com LLM (`ETL 2`):**
    * **Inova√ß√£o:** Em vez de um schema fixo, o script utiliza o LLM para analisar uma amostra dos produtos e inferir quais atributos s√£o relevantes para extra√ß√£o.
    * **Sa√≠da:** `schema.json` (Um contrato de dados din√¢mico).

3.  **Enriquecimento Sem√¢ntico em Lote (`ETL 3`):**
    * **L√≥gica de Batching:** Utiliza a estrat√©gia detalhada no diagrama `Batch LLM.png`. O processamento √© feito em lotes de **10 a 20 produtos**.
    * **Resili√™ncia:** A abordagem incremental evita a perda de progresso em caso de interrup√ß√µes e otimiza o uso da API da OpenAI.
    * **Sa√≠da:** `products_enriched.json`.

4.  **EDA e An√°lise Visual (`EDA Visuals`):**
    * **A√ß√£o:** Script final que consome os dados enriquecidos para gerar insights, gr√°ficos de distribui√ß√£o de atributos e valida√ß√£o da normaliza√ß√£o realizada.

---

## üõ†Ô∏è Stack Tecnol√≥gica

* **Linguagem:** Python (Executado via Google Colab).
* **Orquestra√ß√£o de Dados:** Boto3 (Integra√ß√£o com AWS S3).
* **Intelig√™ncia Artificial:** OpenAI API (Modelo: `gpt-4o-mini`).
* **Processamento & An√°lise:** Pandas, JSONL, Matplotlib, Seaborn.
* **Armazenamento:** AWS S3 Bucket (`dadosfera-datalake`).

---

## üìÇ Organiza√ß√£o dos Arquivos

| Arquivo | Descri√ß√£o |
| :--- | :--- |
| `ETL 1 - Limpeza.ipynb` | Tratamento inicial de 1 milh√£o de produtos. |
| `ETL 2 - Descoberta de Atributos (LLM).ipynb` | Gera√ß√£o autom√°tica do `schema.json`. |
| `ETL 3 - Enriquecimento Sem√¢ntico (LLM).ipynb` | Extra√ß√£o de atributos via GPT em batches. |
| `EDA Visuals.ipynb` | An√°lise explorat√≥ria e visualiza√ß√£o dos dados finais. |
| `Dadosfera ETL LLM.png` | Diagrama principal da arquitetura do pipeline. |
| `Batch LLM.png` | Diagrama do fluxo de processamento em lote para o LLM. |

---

## üöÄ Destaques do Projeto

* **Escalabilidade:** Capaz de lidar com grandes volumes de dados JSONL.
* **Economia de Tokens:** O uso do `gpt-4o-mini` aliado √† limpeza pr√©via reduz drasticamente o custo operacional.
* **Recupera√ß√£o de Falhas:** O sistema de save incremental nos batches garante que o processo possa ser retomado de onde parou.

---
*Este projeto foi desenvolvido como parte de um estudo de caso para a Dadosfera.*
