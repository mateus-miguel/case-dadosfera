# Pipeline de Enriquecimento de Dados com LLM (GPT-4o-mini)

Este projeto implementa um pipeline de ETL (Extract, Transform, Load) inteligente e escal√°vel, focado na normaliza√ß√£o e enriquecimento sem√¢ntico de uma base de dados de produtos (1M+ registros). A solu√ß√£o utiliza LLMs para descoberta din√¢mica de esquemas e processamento em lote para garantir resili√™ncia.

## üèóÔ∏è Arquitetura do Sistema

A solu√ß√£o foi desenhada para operar de forma modular, utilizando o **AWS S3** como camada de persist√™ncia (Data Lake) e o modelo **GPT-4o-mini** da OpenAI como motor de intelig√™ncia.

### Fluxo de Dados (Pipeline)

O processo est√° dividido em quatro etapas principais, conforme ilustrado no diagrama `Dadosfera ETL LLM.png`:

<p align="center">
   <img src='https://raw.githubusercontent.com/mateus-miguel/case-dadosfera/refs/heads/main/metadata/Dadosfera%20ETL%20LLM.png' width='600'/>
</p>

1.  **Limpeza e Normaliza√ß√£o (`ETL 1`):**
    * **Entrada:** `products_raw.jsonl` (Dados brutos).
    * **Processamento:** Limpeza de strings, tratamento de valores nulos e filtragem inicial.
    * **Sa√≠da:** `products_clean.json` (Armazenado no bucket S3 `dadosfera-datalake`).

2.  **Descoberta de Atributos com LLM (`ETL 2`):**
    * **Inova√ß√£o:** Em vez de um schema fixo, o script utiliza o LLM para analisar uma amostra dos produtos e inferir quais atributos s√£o relevantes para extra√ß√£o.
    * **Sa√≠da:** `schema.json` (Um contrato de dados din√¢mico).

3.  **Enriquecimento Sem√¢ntico em Lote (`ETL 3`):**
    * **L√≥gica de Batching:** Utiliza a estrat√©gia detalhada no diagrama `Batch LLM.png`. O processamento √© feito em lotes de **10 a 20 produtos**.
    * **Resili√™ncia:** A abordagem incremental evita a perda de progresso em caso de interrup√ß√µes e otimiza o uso da API da OpenAI.
    * **Sa√≠da:** `products_enriched.json`.

4.  **EDA e An√°lise Visual (`EDA Visuals`):**
    * **A√ß√£o:** Script final que consome os dados enriquecidos para gerar insights, gr√°ficos de distribui√ß√£o de atributos e valida√ß√£o da normaliza√ß√£o realizada.

---

## üßº Limpeza e Normaliza√ß√£o (ETL 1)

O objetivo desta etapa √© realizar o "saneamento" dos dados. Trabalhar com arquivos massivos (mais de 1 milh√£o de registros) em formatos semi-estruturados como `.jsonl` exige um tratamento robusto contra erros de sintaxe e registros incompletos que poderiam comprometer o desempenho do LLM ou gerar custos desnecess√°rios.

### üìã Detalhes do Processo

1.  **Leitura Resiliente (Tratamento de Erros de Sintaxe):**
    * O arquivo original `products_raw.jsonl` continha inconsist√™ncias de formata√ß√£o (valores faltantes, linhas vazias).
    * **Solu√ß√£o:** Implementa√ß√£o de um bloco `try-except` com `json.JSONDecodeError`. Isso permite que o script ignore linhas corrompidas e continue o processamento sem interromper o pipeline, garantindo a integridade da ingest√£o.

2.  **Filtragem de Atributos Essenciais:**
    * Para que o enriquecimento sem√¢ntico funcione, o produto **precisa** ter conte√∫do textual. 
    * **A√ß√£o:** Remo√ß√£o autom√°tica de qualquer registro onde os campos `title` (t√≠tulo) ou `text` (descri√ß√£o/corpo) estivessem vazios ou nulos.

3.  **Normaliza√ß√£o de Codifica√ß√£o:**
    * For√ßamento do padr√£o `utf-8` na leitura e escrita para evitar erros de caracteres especiais (acentua√ß√£o e s√≠mbolos comerciais), comuns em bases de produtos brasileiros.
    * Normaliza√ß√£o por dicion√°rio remove aspas n√£o fechadas, tipogr√°ficas ‚Äú ‚Äù, outros s√≠mbolos irrelevantes como *, ‚úî, ‚û§, ‚Ñ¢, ¬Æ e trechos de marketing como "Product Description", "Next Page".

4.  **Gest√£o de Volume e Amostragem:**
    * Dada a escala de 1M+ de produtos, o script foi configurado para segmentar os dados (ex: processando os primeiros 100.000 registros para valida√ß√£o inicial), permitindo testes de custo-benef√≠cio antes do processamento total via API da OpenAI.

5.  **Persist√™ncia no Data Lake (AWS S3):**
    * O resultado limpo √© exportado para o arquivo `products_clean.json`.
    * O upload √© feito via biblioteca `boto3` diretamente para o bucket `dadosfera-datalake`, servindo como a "Single Source of Truth" (datalake) para os scripts subsequentes de LLM.

### üõ†Ô∏è Especifica√ß√µes T√©cnicas
* **Input:** `s3://dadosfera-datalake/bronze/products_raw.jsonl`
* **Output:** `s3://dadosfera-datalake/silver/products_clean.json`
* **Principais bibliotecas:** `json`, `boto3`, `os`.
* **L√≥gica de Filtro:** `if doc['title'] != '' and doc['text'] != '':`

---

## üîç Descoberta de Atributos (ETL 2)

Nesta etapa, o pipeline utiliza intelig√™ncia artificial para transitar de um dado semi-estruturado para um esquema (schema) rigorosamente definido. Em vez de mapear manualmente centenas de poss√≠veis colunas, o processo utiliza o modelo **GPT-4o-mini** para inferir a estrutura ideal com base no conte√∫do real dos produtos.

### üìã Detalhes do Processo

1.  **Amostragem Inteligente:**
    * O script consome uma amostra representativa do arquivo `products_clean.json` (armazenado no S3). Essa amostra √© enviada ao LLM para que ele entenda a diversidade de categorias e propriedades presentes na base.

2.  **Engenharia de Prompt e Infer√™ncia:**
    * O LLM √© instru√≠do a identificar atributos t√©cnicos (como voltagem, cor, material, dimens√µes, marca) que s√£o recorrentes nas descri√ß√µes.
    * O modelo retorna n√£o apenas o nome do atributo, mas tamb√©m o **tipo de dado** (string, float, integer) e uma **descri√ß√£o funcional** do que aquele campo representa.

3.  **Extra√ß√£o via Regex (Express√µes Regulares):**
    * Como a sa√≠da de um LLM pode conter textos explicativos, o script utiliza **Regex** (`re.findall`) para capturar com precis√£o os padr√µes de atributos, tipos e descri√ß√µes dentro da resposta bruta da API.
    * **L√≥gica de Mapeamento:** Um dicion√°rio `type_mapping` √© utilizado para converter as sugest√µes de tipos do LLM (ex: "texto", "n√∫mero") em tipos Python/JSON v√°lidos (`str`, `float`, `int`).

4.  **Constru√ß√£o do Schema Din√¢mico:**
    * O script consolida uma estrutura base fixa (contendo `id`, `title` e `text`) e anexa a ela todos os novos atributos "descobertos" pela IA.
    * Isso garante que o pipeline seja flex√≠vel: se a base de produtos mudar de "Eletr√¥nicos" para "Moda", o schema se adaptar√° automaticamente sem altera√ß√£o de c√≥digo.

5.  **Persist√™ncia do Contrato de Dados:**
    * O esquema final √© salvo no AWS S3 como `schema.json` (ou `schema_v2.json`). Este arquivo servir√° como o guia mestre para a etapa subsequente de enriquecimento.

### üõ†Ô∏è Especifica√ß√µes T√©cnicas
* **Modelo:** `gpt-4o-mini` (OpenAI).
* **Parsing:** Biblioteca `re` (Regex) para tratamento de strings.
* **Tipagem:** Convers√£o din√¢mica via `type_mapping.get(attr_type.lower(), str)`.
* **Output:** `s3://dadosfera-datalake/metadata/schema.json`.

---

## üß† Enriquecimento Sem√¢ntico (ETL 3)

Esta √© a etapa central de intelig√™ncia do pipeline. Nela, o conte√∫do textual limpo √© transformado em dados estruturados de alto valor, utilizando o modelo **GPT-4o-mini** para extrair informa√ß√µes espec√≠ficas baseadas no esquema definido anteriormente.

### üìã Detalhes do Processo

1.  **Orquestra√ß√£o de Lotes (Batch Processing):**
    * Conforme ilustrado no diagrama `Batch LLM.png`, o processamento n√£o √© feito um a um, o que seria ineficiente. Os produtos s√£o agrupados em **lotes de 10 a 20 itens** por chamada de API.
    * **Objetivo:** Maximizar o uso da janela de contexto do modelo e reduzir a lat√™ncia total do pipeline.

2.  **Estrat√©gia de Resili√™ncia e Salvamento Incremental:**
    * Dado o volume massivo de dados, o script foi desenhado para ser "stateful". Os resultados de cada lote enriquecido s√£o gravados imediatamente ou em intervalos regulares no S3.
    * **Benef√≠cio:** Se houver uma interrup√ß√£o na conex√£o ou limite de taxa (rate limit), o progresso n√£o √© perdido. O pipeline pode ser reiniciado a partir do √∫ltimo ponto de salvamento.

3.  **Extra√ß√£o Guiada por Contexto:**
    * O prompt enviado ao LLM inclui o `schema.json` gerado na etapa 2. Isso for√ßa a IA a devolver apenas os atributos desejados, seguindo rigorosamente os tipos de dados (String, Float, Integer) e descri√ß√µes t√©cnicas.
    * **A√ß√£o:** O modelo atua como um "parser" inteligente, identificando caracter√≠sticas como categoria, material, marca e *features* dentro de descri√ß√µes muitas vezes confusas.

4.  **Monitoramento de Performance:**
    * Implementa√ß√£o de barras de progresso (`tqdm`) para acompanhar o tempo m√©dio de resposta por produto (aprox. 9min por itera√ß√£o de lote) e estimar o tempo total de conclus√£o para a base de 1 milh√£o de produtos.

### üõ†Ô∏è Especifica√ß√µes T√©cnicas
* **Inputs:** `s3://dadosfera-datalake/silver/products_clean.json` e `s3://dadosfera-datalake/metadata/schema.json`.
* **Output:** `s3://dadosfera-datalake/gold/products_enriched.json` (Vers√£o incremental).
* **Motor de IA:** OpenAI `gpt-4o-mini`.
* **Principais bibliotecas:** `openai`, `boto3`, `json`, `tqdm`.

---

## üìä EDA e An√°lise Visual

Ap√≥s o enriquecimento dos dados via LLM, esta etapa final foca na extra√ß√£o de intelig√™ncia e valida√ß√£o da qualidade do pipeline. O objetivo √© transformar os arquivos JSON estruturados em insights visuais e m√©tricas de consist√™ncia.

### üìã Detalhes do Processo

1.  **Carregamento e Unifica√ß√£o de Dados Enriquecidos:**
    * O script consome os diversos arquivos `products_enriched.json` gerados pelo processo de batching no S3.
    * **A√ß√£o:** Consolida√ß√£o em algumas Series Pandas para an√°lise estat√≠stica de certos atributos. Alternativa seria consolida√ß√£o em um √∫nico DataFrame Pandas (mais elaborado e demorado).

2.  **Normaliza√ß√£o de Atributos Extra√≠dos:**
    * Como o LLM pode extrair valores em diferentes formatos, o script realiza uma normaliza√ß√£o final (ex: converter 'EUA', 'US' e 'North America' em formatos √∫nicos como 'United States').
    * **Foco:** Garantir que os atributos descobertos pelo `schema.json` sejam compar√°veis graficamente.

3.  **Visualiza√ß√µes Gr√°ficas:**
    * **Distribui√ß√£o de Pa√≠ses de Origem:** Gr√°ficos de barras mostrando a quantidade de pa√≠ses que mais produzem os produtos.
    * **Pie Chart de Categorias:** An√°lise de porcentagem de macro-categorias √†s quais os produtos pertencem, filtradas das categorias geradas pelo LLM.
    * **Gr√°fico de Barras Agrupado:** An√°lise de produtos a prova d'√°gua agrupados em DataFrame com base nos pa√≠ses de origem.
    * **Gr√°fico de Barras Horizontais de Garantias:** Avalia√ß√£o dos tempos de garantias mais ofertados para os produtos do dataset enriquecido.
    * **Violin Plot de Pre√ßos:** Utilizando dados de macro-categoria e pa√≠ses de origem, a cada produto √© atribu√≠do um valor sint√©tico de pre√ßo para an√°lise da distribui√ß√£o cont√≠nua de valores.

### üõ†Ô∏è Especifica√ß√µes T√©cnicas
* **Input:** `s3://dadosfera-datalake/gold/products_enriched.json` ou `s3://dadosfera-datalake/gold/products_enriched_batch<number>.json`
* **Output:** Dashboard de visualiza√ß√µes e relat√≥rio de qualidade.
* **Principais bibliotecas:** `pandas`, `matplotlib`, `seaborn`, `boto3`.
* **Destaque:** Uso de t√©cnicas de filtragem para lidar com a mem√≥ria do Colab ao processar o volume massivo de dados enriquecidos.

---

## üõ†Ô∏è Stack Tecnol√≥gica

* **Linguagem:** Python (Executado via Google Colab).
* **Orquestra√ß√£o de Dados:** OS, Boto3 (Integra√ß√£o com AWS S3).
* **Intelig√™ncia Artificial:** OpenAI API (Modelo: `gpt-4o-mini`).
* **Processamento & An√°lise:** Pandas, JSONL, Matplotlib, Seaborn.
* **Armazenamento:** AWS S3 Bucket (`dadosfera-datalake`).

---

## üìÇ Organiza√ß√£o dos Arquivos

| Arquivo | Descri√ß√£o |
| :--- | :--- |
| `ETL 1 - Limpeza.ipynb` | Tratamento inicial de 1 milh√£o de produtos. |
| `ETL 2 - Descoberta de Atributos (LLM).ipynb` | Gera√ß√£o autom√°tica do `schema.json`. |
| `ETL 3 - Enriquecimento Sem√¢ntico (LLM).ipynb` | Extra√ß√£o de atributos via GPT em batches. |
| `EDA Visuals.ipynb` | An√°lise explorat√≥ria e visualiza√ß√£o dos dados finais. |
| `Dadosfera ETL LLM.png` | Diagrama principal da arquitetura do pipeline. |
| `Batch LLM.png` | Diagrama do fluxo de processamento em lote para o LLM. |

---

## üöÄ Destaques do Projeto

* **Escalabilidade:** Capaz de lidar com grandes volumes de dados JSONL.
* **Economia de Tokens:** O uso do `gpt-4o-mini` aliado √† limpeza pr√©via reduz drasticamente o custo operacional.
* **Recupera√ß√£o de Falhas:** O sistema de save incremental nos batches garante que o processo possa ser retomado de onde parou.

---
*Este projeto foi desenvolvido como parte de um estudo de caso para a Dadosfera.*
